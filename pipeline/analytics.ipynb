{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Building an Web Logs Analytics Data Pipeline\n",
    "\n",
    "## Before you begin\n",
    "If you’ve ever wanted to learn Python online with streaming data, or data that changes quickly, you may be familiar with the concept of a data pipeline. Data pipelines allow you transform data from one representation to another through a series of steps. \n",
    "In this tutorial, we’re going to walk through building a simple data pipeline with help of Kale.\n",
    "\n",
    "A common use case for a data pipeline is figuring out information about the visitors to your web site. If you’re familiar with Google Analytics, you know the value of seeing real-time and historical information on visitors. In this tutorial, we’ll use data from web server logs to answer questions about our visitors.\n",
    "\n",
    "If you’re unfamiliar, every time you visit a web page, your browser is sent data from a web server. To host this tutorial, we use a high-performance web server called Nginx. Here’s how the process of you typing in a URL and seeing a result works:\n",
    "\n",
    "![image info](./img/process.png)\n",
    "\n",
    "The process of sending a request from a web browser to a server.\n",
    "\n",
    "First, the client sends a request to the web server asking for a certain page. The web server then loads the page from the filesystem and returns it to the client. As it serves the request, the web server writes a line to a log file on the filesystem that contains some metadata about the client and the request. This log enables someone to later see who visited which pages on the website at what time, and perform other analysis.\n",
    "\n",
    "A typical line from the Nginx log could look like this: \n",
    "\n",
    "`X.X.X.X - - [09/Mar/2017:01:15:59 +0000] \"GET /blog/assets/css/jupyter.css HTTP/1.1\" 200 30294 \"http://www.dataquest.io/blog/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/53.0.2785.143 Chrome/53.0.2785.143 Safari/537.36 PingdomPageSpeed/1.0 (pingbot/2.0; +http://www.pingdom.com/)\"`\n",
    "\n",
    "Each request is a single line, and lines are appended in chronological order, as requests are made to the server. The format of each line is the Nginx combined format, below are some descriptions of each variable in this format:\n",
    "\n",
    "- remote_addr: the ip address of the client making the request to the server.\n",
    "- remote_user: if the client authenticated with basic authentication, this is the user name.\n",
    "- time_local: the local time when the request was made. For instance 09/Mar/2017:01:15:59 +0000\n",
    "- request: the type of request, and the URL that it was made to. For instance GET /blog/assets/css/jupyter.css HTTP/1.1 \n",
    "- status: the response status code from the server.\n",
    "- body_bytes_sent: the number of bytes sent by the server to the client in the response body.\n",
    "- http_referrer: the page that the client was on before sending the current request.\n",
    "- http_user_agent: information about the browser and system of the client\n",
    "\n",
    "As you can imagine, companies derive a lot of value from knowing which visitors are on their site, and what they’re doing. For example, realizing that users who use the Google Chrome browser rarely visit a certain page may indicate that the page has a rendering issue in that browser.\n",
    "\n",
    "Another example is in knowing how many users from each district visit your site each day. It can help you figure out which district to focus your marketing efforts on. At the simplest level, just knowing how many visitors you have per day can help you understand if your marketing efforts are working properly.\n",
    "In order to calculate these metrics, we need to parse the log files and analyze them. And to do this, we need to construct a data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline structure\n",
    "\n",
    "Getting from raw logs to browser and status counts per day.\n",
    "\n",
    "![image info](./img/pipeline.png)\n",
    "\n",
    "As you can see above, we go from raw log data to statistical queries where we can see different browser/status counts per day. If necessary, this pipeline can run continuously — when new entries are added to the server log, it grabs them and processes them. There are a few things you’ve hopefully noticed about how we structured the pipeline:\n",
    "\n",
    "- Each pipeline component is separated from the others, and takes in a defined input, and returns a defined output. Each output will be further stored in a Google Storage Bucket to pass the data between pipeline steps. And these cached outputs can be used for further analysis.\n",
    "- We also store the raw log data to a SQLite database. This ensures that if we ever want to run a different analysis, we have access to all of the raw data.\n",
    "- We remove duplicate records. It’s very easy to introduce duplicate data into your analysis process, so deduplicating before passing data through the pipeline is critical.\n",
    "- Each pipeline component feeds data into another component. We want to keep each component as small as possible, so that we can individually scale pipeline components up, or use the outputs for a different type of analysis.\n",
    "\n",
    "Now that we’ve seen how this pipeline looks at a high level, let’s begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating webserver logs\n",
    "In order to create our data pipeline, we’ll need access to webserver log data. \n",
    "In this step we created a script that will continuously generate fake (but somewhat realistic) log data. \n",
    "After running the following cells, you should see new entries being written to logs.txt in the same folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\"\"\"\n",
    "Please note that currently you may have to install the extra packages in this way\n",
    "As \"!\" symbol is not supported yet by Kale\n",
    "\"\"\"\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"Faker==0.7.9\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage==1.24.1\"])\n",
    "\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "LINE = \"\"\"\\\n",
    "{remote_addr} - - [{time_local} +0000] \"{request_type} {request_path} HTTP/1.1\" {status} {body_bytes_sent} \"{http_referer}\" \"{http_user_agent}\"\\\n",
    "\"\"\"\n",
    "\n",
    "LOG_FILE = \"logs.txt\"\n",
    "LOG_MAX = 100 # Define the size of webserver logs to be generated\n",
    "BUCKET = \"web-log-test\" \n",
    "\n",
    "USER = \"hong\" # The name of your sub-directory in the storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_log_line():\n",
    "    fake = Faker()\n",
    "    now = datetime.now()\n",
    "    remote_addr = fake.ipv4()\n",
    "    time_local = now.strftime('%d/%b/%Y:%H:%M:%S')\n",
    "    request_type = random.choice([\"GET\", \"POST\", \"PUT\"])\n",
    "    request_path = \"/\" + fake.uri_path()\n",
    "\n",
    "    status = random.choice([200, 401, 403, 404, 500])\n",
    "    body_bytes_sent = random.choice(range(5, 1000, 1))\n",
    "    http_referer = fake.uri()\n",
    "    http_user_agent = fake.user_agent()\n",
    "\n",
    "    log_line = LINE.format(\n",
    "        remote_addr=remote_addr,\n",
    "        time_local=time_local,\n",
    "        request_type=request_type,\n",
    "        request_path=request_path,\n",
    "        status=status,\n",
    "        body_bytes_sent=body_bytes_sent,\n",
    "        http_referer=http_referer,\n",
    "        http_user_agent=http_user_agent\n",
    "    )\n",
    "    return log_line\n",
    "\n",
    "\n",
    "def write_log_line(log_file, line):\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(line)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "def generate_log_file():\n",
    "    \"\"\"\n",
    "    Generate the weblog file with defined size.\n",
    "    This file will be stored in the given bucket.\n",
    "    \"\"\"\n",
    "    current_log_file = LOG_FILE\n",
    "    lines_written = 0\n",
    "    \n",
    "    while lines_written != LOG_MAX:\n",
    "        line = generate_log_line()\n",
    "        write_log_line(current_log_file, line)\n",
    "        lines_written += 1\n",
    "    print(\"{}{}{}\".format(\"Log file with length \", LOG_MAX, \" successfully generated\"))\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET)\n",
    "    blob = bucket.blob(USER + \"/logs.txt\")\n",
    "    print(\"{}{}{}\".format(\"User directory \", USER, \" created\"))\n",
    "    \n",
    "    with open(current_log_file, \"rb\") as log_file:\n",
    "        blob.upload_from_file(log_file)\n",
    "    print(\"Uploaded web logs data into google storage bucket\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "block:generate_log_file"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file with length 50 successfully generated\n",
      "User directory hong created\n",
      "Uploaded web logs data into google storage bucket\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_log_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Processing and storing webserver logs\n",
    "Once we’ve finished creating the data, we just need to write some code to ingest (or read in) the logs. The script will need to:\n",
    "\n",
    "- Open the log files and read from them line by line.\n",
    "- Parse each line into fields.\n",
    "- Write each line and the parsed fields to a database.\n",
    "- Ensure that duplicate lines aren’t written to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# We picked SQLite in this tutorial because it’s simple, and stores all of the data in a single file. \n",
    "# This enables us to upload the database into a bucket. \n",
    "# If you’re more concerned with performance, you might be better off with a database like Postgres.\n",
    "DB_NAME = \"db.sqlite\"\n",
    "DOWNLOAD_FILE = \"downloaded_logs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    \"\"\"\n",
    "    Create table logs in the SQLite database.\n",
    "    The table schema is defined accroding to the log format.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS logs (\n",
    "      raw_log TEXT NOT NULL,\n",
    "      remote_addr TEXT,\n",
    "      time_local TEXT,\n",
    "      request_type TEXT,\n",
    "      request_path TEXT,\n",
    "      status INTEGER,\n",
    "      body_bytes_sent INTEGER,\n",
    "      http_referer TEXT,\n",
    "      http_user_agent TEXT,\n",
    "      created DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "      )\n",
    "    \"\"\")\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "def parse_line(line):\n",
    "    \"\"\"\n",
    "    Parse each log line by splitting it into structured fields.\n",
    "    Extract all of the fields from the split representation. \n",
    "    \"\"\"\n",
    "    split_line = line.split(\" \")\n",
    "    if len(split_line) < 12:\n",
    "        return []\n",
    "    remote_addr = split_line[0]\n",
    "    time_local = split_line[3] + \" \" + split_line[4]\n",
    "    request_type = split_line[5]\n",
    "    request_path = split_line[6]\n",
    "    status = split_line[8]\n",
    "    body_bytes_sent = split_line[9]\n",
    "    http_referer = split_line[10]\n",
    "    http_user_agent = \" \".join(split_line[11:])\n",
    "    created = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    return [\n",
    "        remote_addr,\n",
    "        time_local,\n",
    "        request_type,\n",
    "        request_path,\n",
    "        status,\n",
    "        body_bytes_sent,\n",
    "        http_referer,\n",
    "        http_user_agent,\n",
    "        created\n",
    "    ]\n",
    "\n",
    "\n",
    "def insert_record(line, parsed):\n",
    "    \"\"\"Insert the parsed records into the logs table of the SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(DB_NAME,timeout=10)\n",
    "    cur = conn.cursor()\n",
    "    args = [line] + parsed # Parsed is a list of the values parsed earlier\n",
    "    cur.execute('INSERT INTO logs VALUES (?,?,?,?,?,?,?,?,?,?)', args)\n",
    "    conn.commit()\n",
    "    conn.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "block:insert_db_data",
     "prev:generate_log_file"
    ]
   },
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)\n",
    "blob = bucket.blob(USER + \"/logs.txt\")\n",
    "\n",
    "with open(\"downloaded_logs.txt\", \"wb\") as log_file:\n",
    "    blob.download_to_file(log_file)\n",
    "    \n",
    "create_table()\n",
    "\n",
    "try:\n",
    "    f = open(\"downloaded_logs.txt\", 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        parsed = parse_line(line.strip())\n",
    "        time.sleep(1)\n",
    "        insert_record(line, parsed)\n",
    "    f.close()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "blob2 = bucket.blob(USER + \"/\" + DB_NAME)\n",
    "blob2.upload_from_filename(DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query data from the database\n",
    "\n",
    "Now we want to consume the data generated by pulling data out of the SQLite database and does some counting by day.\n",
    "\n",
    "In the below code, we:\n",
    "- Connect to the database.\n",
    "- Query any rows that have been added after a certain timestamp.\n",
    "- Fetch all the rows, sorting out unique ips by day.\n",
    "- Count different visitor browsers and HTTP response statuses based on fetched rows.\n",
    "\n",
    "### [TODO] Finish the pipeline design\n",
    "Now you've gained some knowledge about how to define the pipeline with help of Kale extensions. It's time to do some practices. \n",
    "Take your time to read the following cells, and try to finish the last one/two pipeline components. \n",
    "After this, you could complie the pipeline and submit it into the Kubeflow dashboard. \n",
    "\n",
    "Hinweis: The counting tasks could run parallelly as each pipeline component is separated from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "YEAR = 2018\n",
    "MONTH = 9\n",
    "DAY = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def download_db(destination):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET)\n",
    "    blob = bucket.blob(USER + \"/\" + DB_NAME)\n",
    "    \n",
    "    with open(destination, \"wb\") as db:\n",
    "        blob.download_to_file(db)  \n",
    "\n",
    "        \n",
    "def parse_time(time_str):\n",
    "    try:\n",
    "        time_obj = datetime.strptime(time_str, '[%d/%b/%Y:%H:%M:%S %z]')\n",
    "    except Exception:\n",
    "        time_obj = \"\"\n",
    "    return time_obj   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lines_browser(time_obj):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT time_local,http_user_agent FROM logs WHERE created > ?\", [time_obj])\n",
    "    resp = cur.fetchall()\n",
    "    return resp\n",
    "\n",
    "\n",
    "def parse_user_agent(user_agent):\n",
    "    \"\"\"Parsing the user agent to retrieve the name of the browser.\"\"\"\n",
    "    browsers = [\"Firefox\", \"Chrome\", \"Opera\", \"Safari\", \"MSIE\"]\n",
    "    for browser in browsers:\n",
    "        if browser in user_agent:\n",
    "            return browser\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def get_time_and_ip_browser(lines):\n",
    "    \"\"\"Extract the ip and time from each row we queried.\"\"\"\n",
    "    browsers = []\n",
    "    times = []\n",
    "    for line in lines:\n",
    "        times.append(parse_time(line[0]))\n",
    "        browsers.append(parse_user_agent(line[1]))\n",
    "    return browsers, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lines_status(time_obj):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT time_local,status FROM logs WHERE created > ?\", [time_obj])\n",
    "    resp = cur.fetchall()\n",
    "    return resp\n",
    "\n",
    "\n",
    "def parse_response_status(user_status):\n",
    "    \"\"\"\n",
    "    Retrieve the HTTP request response status\n",
    "    200: OK\n",
    "    401: Bad Request\n",
    "    403: Forbidden\n",
    "    404: Not Found\n",
    "    500: Internal Server Error\n",
    "    \"\"\"\n",
    "    statuses = [200, 401, 403, 404, 500]\n",
    "    for status in statuses:\n",
    "        if status == user_status :\n",
    "            return status\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def get_time_and_ip_status(lines):\n",
    "    statuses = []\n",
    "    times = []\n",
    "    for line in lines:\n",
    "        times.append(parse_time(line[0]))\n",
    "        statuses.append(parse_response_status(line[1]))\n",
    "    return statuses, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "block:count_browser",
     "prev:insert_db_data"
    ]
   },
   "outputs": [],
   "source": [
    "download_db(destination=DB_NAME)\n",
    "browser_counts = {}\n",
    "start_time = datetime(year=YEAR, month=MONTH, day=DAY)\n",
    "\n",
    "lines = get_lines_browser(start_time)\n",
    "browsers, times = get_time_and_ip_browser(lines)\n",
    "\n",
    "if len(times) > 0:\n",
    "    start_time = times[-1] \n",
    "for browser, time_obj in zip(browsers, times):\n",
    "    if browser not in browser_counts:\n",
    "        browser_counts[browser] = 0\n",
    "    browser_counts[browser] += 1\n",
    "\n",
    "count_list = browser_counts.items()\n",
    "count_list = sorted(count_list, key=lambda x: x[0])\n",
    "\n",
    "with open('browser_counts.csv','w') as file:\n",
    "        writer = csv.writer(file, delimiter=\",\", lineterminator=\"\\r\\n\")\n",
    "        writer.writerow([\"browser\", \"count\"])\n",
    "        writer.writerows(count_list)\n",
    "        \n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET)     \n",
    "blob = bucket.blob(USER + \"/queries/browser_counts.csv\")\n",
    "blob.upload_from_filename(\"browser_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "block:count_status",
     "prev:insert_db_data"
    ]
   },
   "outputs": [],
   "source": [
    "download_db(destination=DB_NAME)\n",
    "statuses_counts = {}\n",
    "start_time = datetime(year=YEAR, month=MONTH, day=DAY)\n",
    "\n",
    "lines = get_lines_status(start_time)\n",
    "statuses, times = get_time_and_ip_status(lines)\n",
    "\n",
    "if len(times) > 0:\n",
    "    start_time = times[-1] \n",
    "for status, time_obj in zip(statuses, times):\n",
    "    if status not in statuses_counts:\n",
    "        statuses_counts[status] = 0\n",
    "    statuses_counts[status] += 1\n",
    "\n",
    "count_list = statuses_counts.items()\n",
    "count_list = sorted(count_list, key=lambda x: x[0])\n",
    "\n",
    "with open('status_counts.csv','w') as file:\n",
    "        writer = csv.writer(file, delimiter=\",\", lineterminator=\"\\r\\n\")\n",
    "        writer.writerow([\"status\", \"count\"])\n",
    "        writer.writerows(count_list)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(BUCKET) \n",
    "blob = bucket.blob(USER + \"/queries/status_counts.csv\")\n",
    "blob.upload_from_filename(\"status_counts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "gcr.io/idalab-kube/kube-user",
   "experiment": {
    "id": "c4c0fa13-afc1-4612-b1d1-b4382a836ad2",
    "name": "Default"
   },
   "experiment_name": "Default",
   "pipeline_description": "test analytics",
   "pipeline_name": "workshop",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
